from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import requests
import json
import time
import random
import os
import urllib.parse
import re
from typing import List, Dict, Any

print("ğŸš€ å¯åŠ¨æ‰¹é‡Seleniumçˆ¬è™«ï¼ˆè‡ªåŠ¨è·å–åœ°åŒºåˆ—è¡¨å¹¶ç”Ÿæˆåæ ‡æ–‡ä»¶ï¼‰...")

class GeocoderAPI:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def get_coordinates(self, search_query: str) -> Dict:
        """
        ä½¿ç”¨Photon APIè·å–åæ ‡
        """
        url = "https://photon.komoot.io/api/"
        params = {
            'q': search_query,
            'limit': 1
        }
        
        try:
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            if data.get('features') and len(data['features']) > 0:
                feature = data['features'][0]
                coordinates = feature['geometry']['coordinates']
                # Photonè¿”å›çš„æ˜¯[lon, lat]æ ¼å¼
                return {
                    "lat": round(float(coordinates[1]), 6),
                    "lng": round(float(coordinates[0]), 6)
                }
        except Exception as e:
            print(f"Photon APIæŸ¥è¯¢å¤±è´¥ {search_query}: {e}")
        
        return None

def extract_city_name_from_url(url: str) -> str:
    """
    ä»URLä¸­æå–åŸå¸‚åç§°
    ä¾‹å¦‚: "/industries/north-america/american-game-industry/california/san-francisco" -> "San Francisco"
    """
    match = re.search(r'/([^/]+)$', url)
    if match:
        slug = match.group(1)
        city_name = ' '.join(word.capitalize() for word in slug.split('-'))
        return city_name
    return ""

def get_location_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åæå–ä½ç½®ä¿¡æ¯
    ä¾‹å¦‚: "california_cities.json" -> "California, USA"
    """
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    basename = os.path.splitext(filename)[0]
    
    # æå–ä¸»è¦ä½ç½®åç§°ï¼ˆå‡è®¾æ ¼å¼ä¸º"ä½ç½®_cities.json"ï¼‰
    location_match = re.match(r'^([^_]+)', basename)
    if location_match:
        location = location_match.group(1)
        # å°†ä½ç½®åç§°é¦–å­—æ¯å¤§å†™
        location = location.capitalize()
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯ç¾å›½å·åï¼Œæ ¼å¼ä¸º"å·å, USA"
        us_states = [
            'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado',
            'connecticut', 'delaware', 'florida', 'georgia', 'hawaii', 'idaho',
            'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana',
            'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota',
            'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new-hampshire',
            'new-jersey', 'new-mexico', 'new-york', 'north-carolina', 'north-dakota',
            'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode-island', 'south-carolina',
            'south-dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia',
            'washingtone', 'west-virginia', 'wisconsin', 'wyoming'
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥çš„ç¾å›½å·
        if location.lower() in us_states:
            return f"{location}, USA"
        else:
            # å¯¹äºåŠ æ‹¿å¤§çœä»½ï¼Œæ ¼å¼ä¸º"çœä»½, Canada"
            canada_provinces = [
                'ontario', 'quebec', 'british-columbia', 'alberta', 'manitoba',
                'saskatchewan', 'nova-scotia', 'new-brunswick', 'newfoundland-and-labrador',
                'prince-edward-island'
            ]
            if location.lower() in canada_provinces:
                return f"{location}, Canada"
            else:
                # å¯¹äºå…¶ä»–å›½å®¶æˆ–åŸå¸‚ï¼Œç›´æ¥è¿”å›ä½ç½®åç§°
                return f"{location}, Singapore"
    
    # å¦‚æœæ— æ³•ä»æ–‡ä»¶åç¡®å®šä½ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
    return "Canada"

def load_cities_from_json(file_path: str) -> tuple[List[Dict[str, Any]], str]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½åŸå¸‚æ•°æ®ï¼Œå¹¶è¿”å›æ•°æ®å’Œä½ç½®ä¿¡æ¯
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ä»æ–‡ä»¶åè·å–ä½ç½®ä¿¡æ¯
        filename = os.path.basename(file_path)
        location = get_location_from_filename(filename)
        
        cities = []
        for item in data:
            city_name = item.get('name', '')
            city_slug = item.get('slug', '')
            if not city_name:
                city_name = extract_city_name_from_url(item.get('url', ''))
            
            if city_name:
                cities.append({
                    'original_name': city_name,
                    'slug': city_slug,  # ç¡®ä¿åŒ…å«slug
                    'search_name': f"{city_name}, {location}",
                    'location': location,
                    'url': item.get('url', ''),
                    'companies_count': item.get('companies_count', ''),
                    'events_count': item.get('events_count', ''),
                    'jobs_count': item.get('jobs_count', '')
                })
        
        return cities, location
    except Exception as e:
        print(f"åŠ è½½åŸå¸‚æ•°æ®å¤±è´¥: {e}")
        return [], ""

def batch_geocode_cities(cities_data: List[Dict]) -> Dict[str, Dict]:
    """
    æ‰¹é‡æŸ¥è¯¢åŸå¸‚åæ ‡ - ä¿®å¤ï¼šä½¿ç”¨slugä½œä¸ºé”®
    """
    geocoder = GeocoderAPI()
    results = {}
    
    for i, city_info in enumerate(cities_data):
        city_name = city_info['original_name']
        city_slug = city_info['slug']  # ä½¿ç”¨slugä½œä¸ºæ ‡è¯†ç¬¦
        search_name = city_info['search_name']
        
        print(f"æŸ¥è¯¢ä¸­ ({i+1}/{len(cities_data)}): {search_name} (slug: {city_slug})")
        
        coordinates = geocoder.get_coordinates(search_name)
        if coordinates:
            # ä¿®å¤ï¼šä½¿ç”¨slugä½œä¸ºé”®ï¼Œè€Œä¸æ˜¯åŸå¸‚åç§°
            results[city_slug] = coordinates
            print(f"  âœ“ æˆåŠŸ: {coordinates}")
        else:
            print(f"  âœ— å¤±è´¥: {search_name}")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
        if i < len(cities_data) - 1:
            time.sleep(1)
    
    return results

def save_coordinates_to_json(data: Dict, location: str, filename: str = None):
    """
    ä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼çš„JSONæ–‡ä»¶ï¼Œæ ¹æ®ä½ç½®ä¿¡æ¯å‘½åï¼Œå¹¶ä¿å­˜åˆ°coordinatesæ–‡ä»¶å¤¹
    """
    # åˆ›å»ºcoordinatesæ–‡ä»¶å¤¹
    coordinates_dir = "coordinates"
    os.makedirs(coordinates_dir, exist_ok=True)
    
    if not filename:
        # æ ¹æ®ä½ç½®ä¿¡æ¯ç”Ÿæˆæ–‡ä»¶å
        safe_location = location.split(',')[0].lower().replace(' ', '_')
        filename = f"{safe_location}_cities_coordinates.json"
    
    # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(coordinates_dir, filename)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, separators=(',', ': '))
    
    print(f"\nåæ ‡æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
    return file_path

def generate_coordinates_file(cities_json_file: str):
    """
    æ ¹æ®åŸå¸‚JSONæ–‡ä»¶ç”Ÿæˆåæ ‡æ–‡ä»¶
    """
    print(f"\n{'='*60}")
    print("ğŸ—ºï¸  å¼€å§‹ç”ŸæˆåŸå¸‚åæ ‡æ–‡ä»¶")
    print(f"{'='*60}")
    
    cities_data, location = load_cities_from_json(cities_json_file)
    
    if not cities_data:
        print(f"æ— æ³•ä» {cities_json_file} åŠ è½½åŸå¸‚æ•°æ®")
        return None
    
    print(f"ä» {cities_json_file} åŠ è½½äº† {len(cities_data)} ä¸ªåŸå¸‚")
    print(f"æ£€æµ‹åˆ°çš„ä½ç½®: {location}")
    print("å¼€å§‹æ‰¹é‡æŸ¥è¯¢åŸå¸‚ç»çº¬åº¦...")
    print("-" * 50)
    
    # æ‰¹é‡æŸ¥è¯¢
    results = batch_geocode_cities(cities_data)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 50)
    print(f"æŸ¥è¯¢å®Œæˆ: æˆåŠŸ {len(results)}/{len(cities_data)} ä¸ªåŸå¸‚")
    
    # ä¿å­˜ç»“æœåˆ°coordinatesæ–‡ä»¶å¤¹
    coordinates_file = save_coordinates_to_json(results, location)
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœé¢„è§ˆ
    print("\nç»“æœé¢„è§ˆ:")
    print("-" * 30)
    for slug, coords in list(results.items())[:5]:
        print(f'"{slug}": {{ "lat": {coords["lat"]}, "lng": {coords["lng"]} }}')
    
    return coordinates_file

def setup_driver():
    """è®¾ç½®Chromeé©±åŠ¨"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    driver = webdriver.Chrome(options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def get_region_name_from_url(url):
    """ä»URLä¸­æå–åœ°åŒºåç§°ï¼ˆæœ€åä¸€ä¸ªæ–œæ åçš„å•è¯ï¼‰"""
    try:
        # ç§»é™¤æœ«å°¾çš„æ–œæ å¹¶åˆ†å‰²URL
        cleaned_url = url.rstrip('/')
        region_name = cleaned_url.split('/')[-1]
        return region_name
    except Exception as e:
        print(f"âŒ ä»URLæå–åœ°åŒºåç§°å¤±è´¥: {e}")
        return "unknown_region"

def extract_regions_from_country_page(driver, country_url):
    """ä»å›½å®¶é¡µé¢æå–æ‰€æœ‰åœ°åŒºåˆ—è¡¨"""
    try:
        print(f"ğŸŒ è®¿é—®å›½å®¶é¡µé¢è·å–åœ°åŒºåˆ—è¡¨: {country_url}")
        
        driver.get(country_url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        print("âœ… å›½å®¶é¡µé¢åŠ è½½å®Œæˆ")
        time.sleep(3)
        
        # æå–é¡µé¢å†…å®¹
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        regions = []
        
        # æŸ¥æ‰¾æ‰€æœ‰åœ°åŒºå¡ç‰‡ - æ ¹æ®æä¾›çš„HTMLç»“æ„
        region_cards = soup.find_all('a', class_=lambda x: x and 'IndustryCard-root' in str(x) if x else False)
        
        print(f"ğŸ” æ‰¾åˆ° {len(region_cards)} ä¸ªåœ°åŒºå¡ç‰‡")
        
        for card in region_cards:
            try:
                # æå–åœ°åŒºåç§°
                region_name_elem = card.find('span', class_=lambda x: x and 'IndustryCard-cardTitle' in str(x) if x else False)
                if region_name_elem:
                    region_name = region_name_elem.get_text(strip=True)
                    
                    # æå–åœ°åŒºURL
                    region_url = card.get('href', '')
                    if region_url:
                        # æ„å»ºå®Œæ•´URL
                        if not region_url.startswith('http'):
                            region_url = "https://gamecompanies.com" + region_url
                        
                        # ä»URLæå–slug
                        region_slug = region_url.split('/')[-1]
                        
                        region_data = {
                            'name': region_name,
                            'slug': region_slug,  # æ·»åŠ slugå­—æ®µ
                            'url': region_url
                        }
                        
                        # æå–ç»Ÿè®¡ä¿¡æ¯
                        chips = card.find_all('span', class_=lambda x: x and 'MuiChip-label' in str(x) if x else False)
                        for chip in chips:
                            chip_text = chip.get_text(strip=True)
                            if 'companies' in chip_text.lower():
                                region_data['companies_count'] = chip_text
                            elif 'jobs' in chip_text.lower():
                                region_data['jobs_count'] = chip_text
                            elif 'events' in chip_text.lower():
                                region_data['events_count'] = chip_text
                        
                        regions.append(region_data)
                        print(f"  âœ… æå–åœ°åŒº: {region_name} (slug: {region_slug}) - {region_url}")
                        
            except Exception as e:
                print(f"  âŒ å¤„ç†åœ°åŒºå¡ç‰‡æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜åœ°åŒºåˆ—è¡¨
        regions_filename = "turkish_regions.json"
        with open(regions_filename, 'w', encoding='utf-8') as f:
            json.dump(regions, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ åœ°åŒºåˆ—è¡¨å·²ä¿å­˜åˆ° {regions_filename}ï¼Œå…± {len(regions)} ä¸ªåœ°åŒº")
        
        return regions, regions_filename
        
    except Exception as e:
        print(f"âŒ è·å–åœ°åŒºåˆ—è¡¨å¤±è´¥: {e}")
        return [], None

def extract_cities_from_region_page(driver, region_url):
    """ä»åœ°åŒºé¡µé¢æå–æ‰€æœ‰åŸå¸‚åˆ—è¡¨"""
    try:
        # ä»URLè·å–åœ°åŒºåç§°
        region_name = get_region_name_from_url(region_url)
        print(f"ğŸŒ è®¿é—®{region_name}é¡µé¢è·å–åŸå¸‚åˆ—è¡¨...")
        
        driver.get(region_url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        print(f"âœ… {region_name}é¡µé¢åŠ è½½å®Œæˆ")
        time.sleep(3)
        
        # ç‚¹å‡»Load MoreæŒ‰é’®ç›´åˆ°åŠ è½½æ‰€æœ‰åŸå¸‚
        print("ğŸ”„ åŠ è½½æ‰€æœ‰åŸå¸‚...")
        cities_loaded = click_cities_load_more(driver)
        
        # æå–åŸå¸‚ä¿¡æ¯
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        cities = []
        
        # æŸ¥æ‰¾æ‰€æœ‰åŸå¸‚å¡ç‰‡
        city_cards = soup.find_all('a', class_=lambda x: x and 'IndustryCard-root' in x)
        print(f"ğŸ” æ‰¾åˆ° {len(city_cards)} ä¸ªåŸå¸‚å¡ç‰‡")
        
        for card in city_cards:
            try:
                # æå–åŸå¸‚åç§°
                city_name_elem = card.find('span', class_=lambda x: x and 'IndustryCard-cardTitle' in x)
                if city_name_elem:
                    city_name = city_name_elem.get_text(strip=True)
                    
                    # æå–åŸå¸‚URL
                    city_url = card.get('href', '')
                    if city_url:
                        # ä»URLä¸­æå–åŸå¸‚slugï¼ˆæœ€åä¸€ä¸ªéƒ¨åˆ†ï¼‰
                        city_slug = city_url.split('/')[-1]
                        
                        city_data = {
                            'name': city_name,
                            'slug': city_slug,  # è¿™é‡Œå·²ç»æ˜¯è‹±æ–‡slug
                            'url': city_url
                        }
                        
                        # æå–å…¬å¸æ•°é‡ä¿¡æ¯
                        chips = card.find_all('span', class_=lambda x: x and 'MuiChip-label' in x)
                        for chip in chips:
                            chip_text = chip.get_text(strip=True)
                            if 'companies' in chip_text.lower():
                                city_data['companies_count'] = chip_text
                            elif 'jobs' in chip_text.lower():
                                city_data['jobs_count'] = chip_text
                            elif 'events' in chip_text.lower():
                                city_data['events_count'] = chip_text
                        
                        cities.append(city_data)
                        print(f"  âœ… æå–åŸå¸‚: {city_name} (slug: {city_slug})")
                        
            except Exception as e:
                print(f"  âŒ å¤„ç†åŸå¸‚å¡ç‰‡æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿®å¤ï¼šä½¿ç”¨åœ°åŒºçš„slugä½œä¸ºæ–‡ä»¶åï¼ˆä»URLæå–ï¼‰
        region_slug = region_url.split('/')[-1]
        cities_filename = f"{region_slug}_cities.json"
        
        with open(cities_filename, 'w', encoding='utf-8') as f:
            json.dump(cities, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ åŸå¸‚åˆ—è¡¨å·²ä¿å­˜åˆ° {cities_filename}ï¼Œå…± {len(cities)} ä¸ªåŸå¸‚")
        
        return cities, region_name, cities_filename
        
    except Exception as e:
        print(f"âŒ è·å–åŸå¸‚åˆ—è¡¨å¤±è´¥: {e}")
        return [], "unknown_region", None

def click_cities_load_more(driver, max_clicks=20):
    """ç‚¹å‡»åŸå¸‚é¡µé¢çš„Load MoreæŒ‰é’®"""
    click_count = 0
    
    while click_count < max_clicks:
        try:
            # ç­‰å¾…é¡µé¢ç¨³å®š
            time.sleep(2)
            
            # æŸ¥æ‰¾Load MoreæŒ‰é’®
            load_more_button = None
            load_more_selectors = [
                "button:contains('Load more')",
                "//button[contains(text(), 'Load more')]",
                "//button[contains(., 'Load more')]",
                ".GCButton-root"
            ]
            
            for selector in load_more_selectors:
                try:
                    if selector.startswith("//"):
                        load_more_button = driver.find_element(By.XPATH, selector)
                    else:
                        # ä½¿ç”¨JavaScriptæŸ¥æ‰¾åŒ…å«"Load more"æ–‡æœ¬çš„æŒ‰é’®
                        buttons = driver.find_elements(By.TAG_NAME, "button")
                        for button in buttons:
                            if "load more" in button.text.lower():
                                load_more_button = button
                                break
                    
                    if load_more_button:
                        break
                except:
                    continue
            
            if not load_more_button:
                print("âŒ æ‰¾ä¸åˆ°Load MoreæŒ‰é’®ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
                break
            
            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
            if not load_more_button.is_enabled():
                print("âŒ Load MoreæŒ‰é’®ä¸å¯ç‚¹å‡»")
                break
            
            # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
            driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", load_more_button)
            time.sleep(1)
            
            # ç‚¹å‡»æŒ‰é’®
            print(f"ğŸ”„ ç‚¹å‡»ç¬¬ {click_count + 1} æ¬¡Load More...")
            driver.execute_script("arguments[0].click();", load_more_button)
            click_count += 1
            
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            time.sleep(3)
            
            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            print(f"âŒ ç‚¹å‡»Load Moreæ—¶å‡ºé”™: {e}")
            break
    
    print(f"âœ… å…±ç‚¹å‡» {click_count} æ¬¡Load More")
    return click_count

def extract_location_from_url(url):
    """ä»URLä¸­æå–åŸå¸‚å’Œåœ°åŒºä¿¡æ¯"""
    try:
        # è§£æURL
        parsed_url = urllib.parse.urlparse(url)
        path_parts = parsed_url.path.strip('/').split('/')
        
        # æœ€åä¸€ä¸ªéƒ¨åˆ†é€šå¸¸æ˜¯åŸå¸‚slug
        if path_parts:
            city_slug = path_parts[-1]
            
            # æ ¹æ®URLè·¯å¾„æ¨æ–­å›½å®¶ä¿¡æ¯
            country = "åŠ æ‹¿å¤§"  # é»˜è®¤å€¼
            if 'north-america' in url and 'canadian-game-industry' in url:
                country = "åŠ æ‹¿å¤§"
            elif 'north-america' in url and 'american-game-industry' in url:
                country = "ç¾å›½"
            elif 'turkish-game-industry' in url:
                country = "åœŸè€³å…¶"
            elif 'new-zealand-game-industry' in url:
                country = "æ–°è¥¿å…°"
            elif 'brazilian-game-industry' in url:
                country = "å·´è¥¿"
            elif 'chilean-game-industry' in url:
                country = "æ™ºåˆ©"
            elif 'swedish-game-industry' in url:
                country = "ç‘å…¸"
            elif 'german-game-industry' in url:
                country = "å¾·å›½"
            elif 'polish-game-industry' in url:
                country = "æ³¢å…°"
            elif 'french-game-industry' in url:
                country = "æ³•å›½"
            elif 'finnish-game-industry' in url:
                country = "èŠ¬å…°"
            elif 'spanish-game-industry' in url:
                country = "è¥¿ç­ç‰™"
            elif 'dutch-game-industry' in url:
                country = "è·å…°"
            elif 'romanian-game-industry' in url:
                country = "ç½—é©¬å°¼äºš"
            elif 'scottish-game-industry' in url:
                country = "è‹æ ¼å…°"
            elif 'danish-game-industry' in url:
                country = "ä¸¹éº¦"
            elif 'norwegian-game-industry' in url:
                country = "æŒªå¨"
            elif 'ukranian-game-industry' in url:
                country = "ä¹Œå…‹å…°"
            elif 'irish-game-industry' in url:
                country = "çˆ±å°”å…°"
            elif 'czech-game-industry' in url:
                country = "æ·å…‹"
            elif 'italian-game-industry' in url:
                country = "æ„å¤§åˆ©"
            elif 'austrian-game-industry' in url:
                country = "å¥¥åœ°åˆ©"
            elif 'belgian-game-industry' in url:
                country = "æ¯”åˆ©æ—¶"
            elif 'slovakian-game-industry' in url:
                country = "æ–¯æ´›ä¼å…‹"
            elif 'welsh-game-industry' in url:
                country = "å¨å°”å£«"
            elif 'hungarian-game-industry' in url:
                country = "åŒˆç‰™åˆ©"
            elif 'lithuanian-game-industry' in url:
                country = "ç«‹é™¶å®›"
            elif 'maltaic-game-industry' in url:
                country = "é©¬è€³ä»–"
            elif 'northern-irish-game-industry' in url:
                country = "åŒ—çˆ±å°”å…°"
            elif 'english-game-industry' in url:
                country = "è‹±æ ¼å…°"
            elif 'serbian-game-industry' in url:
                country = "å¡å°”ç»´äºš"
            elif 'swiss-game-industry' in url:
                country = "ç‘å£«"
            elif 'belarusian-game-industry' in url:
                country = "ç™½ä¿„ç½—æ–¯"
            elif 'bulgarian-game-industry' in url:
                country = "ä¿åŠ åˆ©äºš"
            elif 'croatian-game-industry' in url:
                country = "å…‹ç½—åœ°äºš"
            elif 'estonian-game-industry' in url:
                country = "çˆ±æ²™å°¼äºš"
            elif 'icelandic-game-industry' in url:
                country = "å†°å²›"
            elif 'portugese-game-industry' in url:
                country = "è‘¡è„ç‰™"
            elif 'slovenian-game-industry' in url:
                country = "æ–¯æ´›æ–‡å°¼äºš"
            elif 'greek-game-industry' in url:
                country = "å¸Œè…Š"
            elif 'latvian-game-industry' in url:
                country = "æ‹‰è„±ç»´äºš"
            elif 'macedonian-game-industry' in url:
                country = "åŒ—é©¬å…¶é¡¿"
            elif 'moldovian-game-industry' in url:
                country = "æ‘©å°”å¤šç“¦"
            elif 'singaporean-game-industry' in url:
                country = "æ–°åŠ å¡"
            
            return city_slug, country  # è¿”å›slugè€Œä¸æ˜¯åŸå¸‚åç§°
        else:
            return "unknown", "æœªçŸ¥å›½å®¶"
            
    except Exception as e:
        print(f"âŒ ä»URLæå–ä½ç½®ä¿¡æ¯å¤±è´¥: {e}")
        return "unknown", "æœªçŸ¥å›½å®¶"

def click_load_more(driver, max_clicks=50):
    """ç‚¹å‡»Load MoreæŒ‰é’®ç›´åˆ°æ²¡æœ‰æ›´å¤šæ•°æ®"""
    click_count = 0
    total_companies = 0
    
    while click_count < max_clicks:
        try:
            # ç­‰å¾…é¡µé¢ç¨³å®š
            time.sleep(2)
            
            # æŸ¥æ‰¾Load MoreæŒ‰é’®
            load_more_selectors = [
                "button[aria-label*='load more']",
                "button:contains('Load more')",
                "//button[contains(text(), 'Load more')]",
                "//button[contains(., 'Load more')]",
                ".MuiButton-root",
                "button"
            ]
            
            load_more_button = None
            
            # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
            for selector in load_more_selectors:
                try:
                    if selector.startswith("//"):
                        load_more_button = driver.find_element(By.XPATH, selector)
                    else:
                        load_more_button = driver.find_element(By.CSS_SELECTOR, selector)
                    
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦åŒ…å«"Load more"æ–‡æœ¬
                    if load_more_button and "load more" in load_more_button.text.lower():
                        break
                    else:
                        load_more_button = None
                except:
                    continue
            
            if not load_more_button:
                print("âŒ æ‰¾ä¸åˆ°Load MoreæŒ‰é’®")
                break
            
            # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
            driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", load_more_button)
            time.sleep(1)
            
            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
            if not load_more_button.is_enabled():
                print("âŒ Load MoreæŒ‰é’®ä¸å¯ç‚¹å‡»")
                break
            
            # ç‚¹å‡»æŒ‰é’®
            print(f"ğŸ”„ ç‚¹å‡»ç¬¬ {click_count + 1} æ¬¡Load More...")
            driver.execute_script("arguments[0].click();", load_more_button)
            click_count += 1
            
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å…¬å¸åŠ è½½
            current_companies = count_companies(driver)
            print(f"ğŸ“Š å½“å‰å…¬å¸æ•°é‡: {current_companies}")
            
            # å¦‚æœå…¬å¸æ•°é‡æ²¡æœ‰å¢åŠ ï¼Œå¯èƒ½å·²ç»åŠ è½½å®Œæ¯•
            if current_companies <= total_companies:
                print("âš ï¸ å…¬å¸æ•°é‡æ²¡æœ‰å¢åŠ ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
                break
            
            total_companies = current_companies
            
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«æ£€æµ‹
            time.sleep(random.uniform(2, 4))
            
        except TimeoutException:
            print("â° ç­‰å¾…è¶…æ—¶ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
            break
        except NoSuchElementException:
            print("âŒ æ‰¾ä¸åˆ°Load MoreæŒ‰é’®ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
            break
        except Exception as e:
            print(f"âŒ ç‚¹å‡»Load Moreæ—¶å‡ºé”™: {e}")
            break
    
    print(f"âœ… å…±ç‚¹å‡» {click_count} æ¬¡Load More")
    return click_count

def count_companies(driver):
    """è®¡ç®—å½“å‰é¡µé¢ä¸Šçš„å…¬å¸æ•°é‡"""
    try:
        company_selectors = [
            '[class*="ItemListItem-root"]',
            '[class*="company"]',
            '[class*="card"]'
        ]
        
        for selector in company_selectors:
            companies = driver.find_elements(By.CSS_SELECTOR, selector)
            if companies:
                return len(companies)
        
        return 0
    except:
        return 0

def extract_companies(driver):
    """ä»é¡µé¢ä¸­æå–å…¬å¸æ•°æ®"""
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')
    
    companies = []
    seen_names = set()
    
    # æŸ¥æ‰¾æ‰€æœ‰å…¬å¸å…ƒç´ 
    company_elements = soup.find_all(['div', 'a'], class_=lambda x: x and any(cls in str(x) for cls in ['ItemListItem', 'company', 'card']))
    
    print(f"ğŸ” æ‰¾åˆ° {len(company_elements)} ä¸ªå…¬å¸å…ƒç´ ")
    
    for element in company_elements:
        try:
            company_data = {}
            
            # æå–å…¬å¸åç§°
            name_selectors = [
                '[class*="ItemListItem-title"]',
                '[class*="title"]',
                '[class*="name"]',
                'h1', 'h2', 'h3', 'h4', 'h5', 'h6'
            ]
            
            name = None
            for selector in name_selectors:
                name_elem = element.select_one(selector)
                if name_elem and name_elem.get_text(strip=True):
                    name = name_elem.get_text(strip=True)
                    break
            
            if not name or name in seen_names:
                continue
                
            seen_names.add(name)
            company_data['name'] = name
            
            # æå–æè¿°
            desc_selectors = [
                '[class*="ItemListItem-subtitle"]',
                '[class*="description"]',
                '[class*="body"]',
                'p'
            ]
            
            for selector in desc_selectors:
                desc_elem = element.select_one(selector)
                if desc_elem and desc_elem.get_text(strip=True):
                    company_data['description'] = desc_elem.get_text(strip=True)
                    break
            
            # æå–å›¾ç‰‡
            img_elem = element.find('img')
            if img_elem:
                src = img_elem.get('src', '')
                if src and not src.startswith('data:'):
                    company_data['image_url'] = src
                
                # æå–srcset
                srcset = img_elem.get('srcset', '')
                if srcset:
                    company_data['image_srcset'] = srcset
            
            # æå–æ ‡ç­¾
            tags = []
            tag_elements = element.find_all(class_=lambda x: x and 'MuiChip-label' in x)
            for tag in tag_elements:
                tag_text = tag.get_text(strip=True)
                if tag_text and tag_text not in ['Studio', 'Publisher', 'Indie']:  # è¿‡æ»¤é€šç”¨æ ‡ç­¾
                    tags.append(tag_text)
            company_data['tags'] = tags
            
            # æå–é“¾æ¥
            if element.name == 'a':
                company_data['link'] = element.get('href', '')
            else:
                link_elem = element.find('a')
                if link_elem:
                    company_data['link'] = link_elem.get('href', '')
            
            companies.append(company_data)
            print(f"  âœ… æå–: {name}")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å…ƒç´ æ—¶å‡ºé”™: {e}")
            continue
    
    return companies

def process_companies_data(companies_data, city_slug, country):
    """å¤„ç†å…¬å¸æ•°æ®ï¼šåˆ é™¤ä¸éœ€è¦çš„å­—æ®µï¼Œæ·»åŠ åŸå¸‚å’Œå›½å®¶ä¿¡æ¯"""
    processed_data = []
    
    for item in companies_data:
        # åˆ›å»ºæ–°å¯¹è±¡ï¼Œåªä¿ç•™åç§°
        processed_item = {
            'name': item.get('name', '')
        }
        
        # æ·»åŠ ä½ç½®ä¿¡æ¯ - ä½¿ç”¨slugä½œä¸ºåŸå¸‚æ ‡è¯†
        processed_item['city'] = city_slug  # ä½¿ç”¨slugè€Œä¸æ˜¯åŸå¸‚åç§°
        processed_item['country'] = country
        
        processed_data.append(processed_item)
    
    return processed_data

def save_processed_data(processed_data, city_slug, country, output_dir='processed_companies'):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹
    os.makedirs(output_dir, exist_ok=True)
    
    # æ„å»ºæ–°çš„æ–‡ä»¶å - ä½¿ç”¨slug
    if processed_data:
        # ä½¿ç”¨åŸå¸‚slugå’Œå›½å®¶åç§°åˆ›å»ºæ–‡ä»¶å
        filename = f"{country}_{city_slug}_game_companies.json".replace(' ', '_')
    else:
        filename = "game_companies.json"
    
    # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(output_dir, filename)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    with open(file_path, 'w', encoding='utf-8') as file:
        json.dump(processed_data, file, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ å¤„ç†åçš„æ•°æ®å·²ä¿å­˜ä¸º {file_path}")
    return file_path

def scrape_single_city(city_data, base_url="https://gamecompanies.com"):
    """çˆ¬å–å•ä¸ªåŸå¸‚çš„å…¬å¸æ•°æ®"""
    # æ„å»ºå®Œæ•´URL
    if city_data['url'].startswith('http'):
        target_url = city_data['url']
    else:
        target_url = base_url + city_data['url']
    
    # ä»URLæå–ä½ç½®ä¿¡æ¯ - ç°åœ¨è¿”å›slug
    city_slug, country = extract_location_from_url(target_url)
    print(f"ğŸ“ æ£€æµ‹åˆ°ä½ç½®ä¿¡æ¯: {city_slug}, {country}")
    
    driver = setup_driver()
    all_companies = []
    
    try:
        print(f"ğŸŒ è®¿é—®: {target_url}")
        
        driver.get(target_url)
        
        # ç­‰å¾…åˆå§‹é¡µé¢åŠ è½½
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        print("âœ… åˆå§‹é¡µé¢åŠ è½½å®Œæˆ")
        time.sleep(3)
        
        # åˆå§‹å…¬å¸æ•°é‡
        initial_count = count_companies(driver)
        print(f"ğŸ“Š åˆå§‹å…¬å¸æ•°é‡: {initial_count}")
        
        # ç‚¹å‡»Load MoreåŠ è½½æ‰€æœ‰æ•°æ®
        print("ğŸ”„ å¼€å§‹åŠ è½½æ›´å¤šæ•°æ®...")
        clicks = click_load_more(driver, max_clicks=40)  # æœ€å¤šç‚¹å‡»40æ¬¡
        
        # æœ€ç»ˆå…¬å¸æ•°é‡
        final_count = count_companies(driver)
        print(f"ğŸ“Š æœ€ç»ˆå…¬å¸æ•°é‡: {final_count}")
        print(f"ğŸ“ˆ å…±åŠ è½½äº† {final_count - initial_count} æ¡æ–°æ•°æ®")
        
        # æå–æ‰€æœ‰å…¬å¸æ•°æ®
        print("ğŸ” æå–å…¬å¸æ•°æ®...")
        all_companies = extract_companies(driver)
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return [], None
    finally:
        driver.quit()
        print("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
    
    # å¤„ç†æ•°æ®
    if all_companies:
        print("ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®...")
        processed_companies = process_companies_data(all_companies, city_slug, country)
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        output_path = save_processed_data(processed_companies, city_slug, country)
        
        return processed_companies, output_path
    else:
        return [], None

def batch_scrape_all_cities(region_url, max_cities=None):
    """æ‰¹é‡çˆ¬å–æŒ‡å®šåœ°åŒºçš„æ‰€æœ‰åŸå¸‚æ•°æ®"""
    driver = setup_driver()
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰åŸå¸‚åˆ—è¡¨
        print("=" * 60)
        print(f"ğŸ™ï¸  ç¬¬ä¸€æ­¥ï¼šè·å–{get_region_name_from_url(region_url)}æ‰€æœ‰åŸå¸‚åˆ—è¡¨")
        print("=" * 60)
        
        cities, region_name, cities_filename = extract_cities_from_region_page(driver, region_url)
        
        if not cities:
            print("âŒ æœªèƒ½è·å–åŸå¸‚åˆ—è¡¨")
            return {}
        
        # é™åˆ¶å¤„ç†çš„åŸå¸‚æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_cities and max_cities < len(cities):
            cities = cities[:max_cities]
            print(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰ {max_cities} ä¸ªåŸå¸‚")
        
    finally:
        driver.quit()
    
    # ç¬¬äºŒæ­¥ï¼šé€ä¸ªçˆ¬å–æ¯ä¸ªåŸå¸‚
    print(f"\n{'='*60}")
    print("ğŸ¢ ç¬¬äºŒæ­¥ï¼šæ‰¹é‡çˆ¬å–å„åŸå¸‚å…¬å¸æ•°æ®")
    print(f"{'='*60}")
    
    results = {}
    successful_cities = 0
    
    for i, city_data in enumerate(cities, 1):
        print(f"\nğŸ™ï¸  å¼€å§‹å¤„ç†ç¬¬ {i}/{len(cities)} ä¸ªåŸå¸‚: {city_data['name']} (slug: {city_data['slug']})")
        print(f"   åŸå¸‚ä¿¡æ¯: {city_data.get('companies_count', 'æœªçŸ¥')}")
        
        # çˆ¬å–å•ä¸ªåŸå¸‚
        processed_companies, output_path = scrape_single_city(city_data)
        
        # è®°å½•ç»“æœ - ä½¿ç”¨slugä½œä¸ºé”®
        results[city_data['slug']] = {
            'name': city_data['name'],  # ä¿ç•™åŸå§‹åç§°ç”¨äºæ˜¾ç¤º
            'companies_count': len(processed_companies),
            'output_path': output_path,
            'original_info': city_data.get('companies_count', 'æœªçŸ¥')
        }
        
        if len(processed_companies) > 0:
            successful_cities += 1
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        if i < len(cities):
            delay = random.uniform(8, 15)
            print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåŸå¸‚...")
            time.sleep(delay)
    
    return results, successful_cities, len(cities), region_name, cities_filename

def batch_scrape_all_regions(country_url, max_regions=None):
    """æ‰¹é‡çˆ¬å–æŒ‡å®šå›½å®¶çš„æ‰€æœ‰åœ°åŒºæ•°æ®"""
    driver = setup_driver()
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰åœ°åŒºåˆ—è¡¨
        print("=" * 60)
        print("ğŸï¸  ç¬¬ä¸€æ­¥ï¼šè·å–åœŸè€³å…¶æ‰€æœ‰åœ°åŒºåˆ—è¡¨")
        print("=" * 60)
        
        regions, regions_filename = extract_regions_from_country_page(driver, country_url)
        
        if not regions:
            print("âŒ æœªèƒ½è·å–åœ°åŒºåˆ—è¡¨")
            return {}
        
        # é™åˆ¶å¤„ç†çš„åœ°åŒºæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_regions and max_regions < len(regions):
            regions = regions[:max_regions]
            print(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰ {max_regions} ä¸ªåœ°åŒº")
        
    finally:
        driver.quit()
    
    # ç¬¬äºŒæ­¥ï¼šé€ä¸ªçˆ¬å–æ¯ä¸ªåœ°åŒºçš„åŸå¸‚
    print(f"\n{'='*60}")
    print("ğŸ™ï¸  ç¬¬äºŒæ­¥ï¼šæ‰¹é‡çˆ¬å–å„åœ°åŒºåŸå¸‚æ•°æ®")
    print(f"{'='*60}")
    
    all_results = {}
    total_cities_processed = 0
    successful_regions = 0
    
    for i, region_data in enumerate(regions, 1):
        print(f"\nğŸï¸  å¼€å§‹å¤„ç†ç¬¬ {i}/{len(regions)} ä¸ªåœ°åŒº: {region_data['name']} (slug: {region_data['slug']})")
        print(f"   åœ°åŒºä¿¡æ¯: {region_data.get('companies_count', 'æœªçŸ¥')}")
        print(f"   åœ°åŒºURL: {region_data['url']}")
        
        # çˆ¬å–å•ä¸ªåœ°åŒºçš„åŸå¸‚
        region_results, successful_count, total_count, region_name, cities_filename = batch_scrape_all_cities(
            region_url=region_data['url'], 
            max_cities=None  # è®¾ç½®ä¸ºNoneçˆ¬å–æ‰€æœ‰åŸå¸‚
        )
        
        # è®°å½•ç»“æœ - ä½¿ç”¨slugä½œä¸ºé”®
        all_results[region_data['slug']] = {
            'name': region_data['name'],  # ä¿ç•™åŸå§‹åç§°ç”¨äºæ˜¾ç¤º
            'url': region_data['url'],
            'cities_count': total_count,
            'successful_cities': successful_count,
            'region_results': region_results
        }
        
        total_cities_processed += total_count
        
        if successful_count > 0:
            successful_regions += 1
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        if i < len(regions):
            delay = random.uniform(10, 20)
            print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåœ°åŒº...")
            time.sleep(delay)
    
    return all_results, successful_regions, len(regions), total_cities_processed, regions_filename

# è¿è¡Œå®Œæ•´æµç¨‹
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è‡ªåŠ¨è·å–åœ°åŒºåˆ—è¡¨å¹¶æ‰¹é‡çˆ¬å–...")
    
    # è®¾ç½®è¦çˆ¬å–çš„å›½å®¶URL
    country_url = "https://gamecompanies.com/industries/asia/singaporean-game-industry"
    
    # æ‰¹é‡çˆ¬å–æ‰€æœ‰åœ°åŒºï¼ˆå¯ä»¥è®¾ç½®max_regionsæ¥é™åˆ¶æ•°é‡è¿›è¡Œæµ‹è¯•ï¼‰
    all_region_results, successful_region_count, total_region_count, total_cities_count, regions_filename = batch_scrape_all_regions(
        country_url=country_url, 
        max_regions=None  # è®¾ç½®ä¸ºNoneçˆ¬å–æ‰€æœ‰åœ°åŒº
    )
    
    # æ±‡æ€»ç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ“Š æ‰¹é‡çˆ¬å–æ±‡æ€»ç»“æœ")
    print(f"{'='*60}")
    
    total_companies_all_regions = 0
    
    for region_slug, region_result in all_region_results.items():
        region_companies = 0
        for city_slug, city_result in region_result['region_results'].items():
            region_companies += city_result['companies_count']
        
        total_companies_all_regions += region_companies
        
        status = "âœ… æˆåŠŸ" if region_result['successful_cities'] > 0 else "âŒ å¤±è´¥"
        region_name = region_result['name']
        print(f"{region_name:25} : {status} - {region_result['successful_cities']:2d}/{region_result['cities_count']:2d} ä¸ªåŸå¸‚, {region_companies:4d} å®¶å…¬å¸")
    
    print(f"\nğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆï¼")
    print(f"   å›½å®¶: åœŸè€³å…¶")
    print(f"   æˆåŠŸçˆ¬å–: {successful_region_count}/{total_region_count} ä¸ªåœ°åŒº")
    print(f"   æ€»åŸå¸‚æ•°: {total_cities_count}")
    print(f"   æ€»å…¬å¸æ•°: {total_companies_all_regions}")
    print(f"   æ•°æ®ä¿å­˜ä½ç½®: processed_companies/ æ–‡ä»¶å¤¹")
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_file = "turkish_scraping_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_region_results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜ä¸º: {summary_file}")
    
    # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    regions_with_companies = [slug for slug, result in all_region_results.items() 
                             if any(city['companies_count'] > 0 for city in result['region_results'].values())]
    
    if regions_with_companies:
        print(f"   æœ‰å…¬å¸çš„åœ°åŒº: {len(regions_with_companies)} ä¸ª")
        avg_companies_per_region = total_companies_all_regions / len(regions_with_companies) if regions_with_companies else 0
        print(f"   å¹³å‡æ¯ä¸ªåœ°åŒºå…¬å¸æ•°: {avg_companies_per_region:.1f}")
        
        # æ˜¾ç¤ºå…¬å¸æœ€å¤šçš„å‰5ä¸ªåœ°åŒº
        top_regions = sorted(all_region_results.items(), 
                           key=lambda x: sum(city['companies_count'] for city in x[1]['region_results'].values()), 
                           reverse=True)[:5]
        print(f"   å…¬å¸æœ€å¤šçš„å‰5ä¸ªåœ°åŒº:")
        for region_slug, result in top_regions:
            region_companies = sum(city['companies_count'] for city in result['region_results'].values())
            region_name = result['name']
            print(f"     {region_name}: {region_companies} å®¶å…¬å¸")
    
    # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ‰€æœ‰åœ°åŒºç”Ÿæˆåæ ‡æ–‡ä»¶
    print(f"\n{'='*60}")
    print("ğŸ—ºï¸  ç¬¬ä¸‰æ­¥ï¼šä¸ºæ‰€æœ‰åœ°åŒºç”Ÿæˆåæ ‡æ–‡ä»¶")
    print(f"{'='*60}")
    
    coordinates_files = []
    
    for region_slug, region_result in all_region_results.items():
        cities_filename = f"{region_slug}_cities.json"
        
        if os.path.exists(cities_filename):
            print(f"\nğŸŒ ä¸º {region_result['name']} ç”Ÿæˆåæ ‡æ–‡ä»¶...")
            coordinates_file = generate_coordinates_file(cities_filename)
            if coordinates_file:
                coordinates_files.append(coordinates_file)
                print(f"âœ… åæ ‡æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {coordinates_file}")
            else:
                print(f"âŒ åæ ‡æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {region_result['name']}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°åŸå¸‚æ•°æ®æ–‡ä»¶: {cities_filename}")
            # è°ƒè¯•ä¿¡æ¯ï¼šåˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶
            all_files = [f for f in os.listdir('.') if f.endswith('_cities.json')]
            print(f"   å½“å‰ç›®å½•ä¸‹çš„citiesæ–‡ä»¶: {all_files}")
    
    print(f"\nğŸ¯ åæ ‡æ–‡ä»¶ç”Ÿæˆç»Ÿè®¡: æˆåŠŸ {len(coordinates_files)} ä¸ªåœ°åŒº")

    # ç¬¬å››æ­¥ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print(f"\n{'='*60}")
    print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
    print(f"{'='*60}")
    
    files_to_delete = []
    
    # æ·»åŠ è¦åˆ é™¤çš„åœ°åŒºæ–‡ä»¶
    if regions_filename and os.path.exists(regions_filename):
        files_to_delete.append(regions_filename)
        print(f"ğŸ—‘ï¸  å¾…åˆ é™¤åœ°åŒºæ–‡ä»¶: {regions_filename}")
    
    # æ·»åŠ è¦åˆ é™¤çš„æ±‡æ€»æ–‡ä»¶
    if os.path.exists(summary_file):
        files_to_delete.append(summary_file)
        print(f"ğŸ—‘ï¸  å¾…åˆ é™¤æ±‡æ€»æ–‡ä»¶: {summary_file}")
    
    # æ·»åŠ è¦åˆ é™¤çš„åŸå¸‚æ–‡ä»¶ - ä½¿ç”¨slug
    for region_slug, region_result in all_region_results.items():
        cities_filename = f"{region_slug}_cities.json"
        if os.path.exists(cities_filename):
            files_to_delete.append(cities_filename)
            print(f"ğŸ—‘ï¸  å¾…åˆ é™¤åŸå¸‚æ–‡ä»¶: {cities_filename}")
    
    # åˆ é™¤æ–‡ä»¶
    deleted_count = 0
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            print(f"âœ… å·²åˆ é™¤: {file_path}")
            deleted_count += 1
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
    
    print(f"ğŸ¯ æ¸…ç†å®Œæˆ: æˆåŠŸåˆ é™¤ {deleted_count}/{len(files_to_delete)} ä¸ªä¸´æ—¶æ–‡ä»¶")